Arguments in args:
{'conf': './config/train.yml'}
Arguments in yaml:
{'datasets': {'dataloader_setting': {'aux_segment_length': 4,
                                     'batch_size': 64,
                                     'num_workers': 16,
                                     'repeat': 1,
                                     'sample_rate': 16000,
                                     'segment_length': 4},
              'test': {'clean_scp': '/home/work_nfs4_ssd/xpyan/Extr/minidata/tt/minidata_clean_cv.lst',
                       'clean_spk': '/home/work_nfs4_ssd/xpyan/Extr/minidata/tt/minidata_clean_cv_id.lst',
                       'infer_scp': '/home/work_nfs4_ssd/xpyan/Extr/minidata/tt/minidata_clean_cv.lst',
                       'noise_scp': '/home/work_nfs4_ssd/ykjv/data/new_DNS/data_list/cv/new_noise_cv.lst',
                       'rir_scp': '/home/work_nfs4_ssd/ykjv/data/new_DNS/data_list/cv/new_rir_cv.lst'},
              'train': {'clean_scp': '/home/work_nfs4_ssd/xpyan/Extr/minidata/tr/minidata_clean_tr.lst',
                        'clean_spk': '/home/work_nfs4_ssd/xpyan/Extr/minidata/tr/minidata_clean_tr_id.lst',
                        'infer_scp': '/home/work_nfs4_ssd/xpyan/Extr/minidata/tr/minidata_clean_tr.lst',
                        'noise_scp': '/home/work_nfs4_ssd/ykjv/data/new_DNS/data_list/tr/new_noise_tr.lst',
                        'rir_scp': '/home/work_nfs5_ssd/zqwang/workspace/dccrn/scp/total/train/new_rir_tr.lst'},
              'val': {'clean_scp': '/home/work_nfs4_ssd/xpyan/Extr/minidata/cv/minidata_clean_cv.lst',
                      'clean_spk': '/home/work_nfs4_ssd/xpyan/Extr/minidata/cv/minidata_clean_cv_id.lst',
                      'infer_scp': '/home/work_nfs4_ssd/xpyan/Extr/minidata/cv/minidata_clean_cv.lst',
                      'noise_scp': '/home/work_nfs4_ssd/ykjv/data/new_DNS/data_list/cv/new_noise_cv.lst',
                      'rir_scp': '/home/work_nfs5_ssd/zqwang/workspace/dccrn/scp/total/eval/new_rir_cv.lst'}},
 'logger': {'path': None, 'print_freq': 100},
 'nnet_conf': {'filter_num': 2048,
               'frame_hop': 160,
               'frame_len': 320,
               'linear_dim': 1024,
               'lstm_dim': 256},
 'optim': {'gradient_clip': 5.0,
           'lr_attention': 1e-06,
           'name': 'adam',
           'optimizer_kwargs': {'lr': 0.001, 'weight_decay': 1e-05}},
 'scheduler': {'T_max': 50, 'eta_min': 0, 'last_epoch': -1},
 'test': False,
 'train': {'checkpoint': '/home/work_nfs7/zqwang/summer_intern_dir/team03/summer_intern/exp/exp_cos_2loss_TAPloss',
           'early_stop': 10,
           'epoch': 200,
           'eval_interval': -1,
           'resume': None,
           'save_period': 1,
           'seed': 1234,
           'spk_resume': '/home/work_nfs4_ssd/hzhao/aslp-spknet-fork/exp/ecapa_augment_vox2/results/ecapa_augments_vox2/final.pth.tar',
           'use_cuda': True}}
Trainer: model summary:
E3Net(
  (spk_model): ModelWrapper(
    (feature_extractor): FeatureWrapper(
      (feature_module): Fbank(
        (stft): STFT()
        (filterbank): Filterbank()
      )
      (feature_normalization): SentenceFeatureNormalization()
    )
    (embedding_model): EcapaTdnnSpeakerVerification(
      (conv_block): TdnnConvLayer(
        (activation): ReLU()
        (kernel): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))
        (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se_res2block_list): ModuleList(
        (0): SERes2Block(
          (block1): TdnnConvLayer(
            (activation): ReLU()
            (kernel): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (dilated_conv): Res2DilatedConv1d(
            (conv_list): ModuleList(
              (0): Identity()
              (1): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (3): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (4): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (5): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (6): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (7): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
          )
          (block2): TdnnConvLayer(
            (activation): ReLU()
            (kernel): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (se_block): SEBlock1d(
            (avg_pool): AdaptiveAvgPool1d(output_size=1)
            (fc): Sequential(
              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
              (1): ReLU(inplace=True)
              (2): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
              (3): Sigmoid()
            )
          )
        )
        (1): SERes2Block(
          (block1): TdnnConvLayer(
            (activation): ReLU()
            (kernel): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (dilated_conv): Res2DilatedConv1d(
            (conv_list): ModuleList(
              (0): Identity()
              (1): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (3): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (4): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (5): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (6): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (7): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
          )
          (block2): TdnnConvLayer(
            (activation): ReLU()
            (kernel): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (se_block): SEBlock1d(
            (avg_pool): AdaptiveAvgPool1d(output_size=1)
            (fc): Sequential(
              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
              (1): ReLU(inplace=True)
              (2): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
              (3): Sigmoid()
            )
          )
        )
        (2): SERes2Block(
          (block1): TdnnConvLayer(
            (activation): ReLU()
            (kernel): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (dilated_conv): Res2DilatedConv1d(
            (conv_list): ModuleList(
              (0): Identity()
              (1): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (3): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (4): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (5): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (6): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (7): TdnnConvLayer(
                (activation): ReLU()
                (kernel): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
                (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
          )
          (block2): TdnnConvLayer(
            (activation): ReLU()
            (kernel): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (se_block): SEBlock1d(
            (avg_pool): AdaptiveAvgPool1d(output_size=1)
            (fc): Sequential(
              (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
              (1): ReLU(inplace=True)
              (2): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
              (3): Sigmoid()
            )
          )
        )
      )
      (mfa): TdnnConvLayer(
        (activation): ReLU()
        (kernel): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))
        (norm): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (attentive_statistic_pooling): AttentiveStatisticsPooling(
        (tdnn): TdnnConvLayer(
          (activation): ReLU()
          (kernel): Conv1d(4608, 128, kernel_size=(1,), stride=(1,))
          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (tanh): Tanh()
        (conv): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))
      )
      (asp_bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (fc): Conv1d(3072, 256, kernel_size=(1,), stride=(1,))
    )
  )
  (encoder): Conv1d(1, 2048, kernel_size=(320,), stride=(160,))
  (encoder_norm): Sequential(
    (0): PReLU(num_parameters=1)
    (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (fullyconnection): Sequential(
    (0): Linear(in_features=2304, out_features=1024, bias=True)
    (1): PReLU(num_parameters=1)
    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (lstmblock_1): LSTMBlock(
    (fullyconnection_block): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): PReLU(num_parameters=1)
      (2): Linear(in_features=1024, out_features=256, bias=True)
      (3): PReLU(num_parameters=1)
      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (lstm): LSTM(256, 256, batch_first=True)
    (layernorm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (layernorm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lstmblock_2): LSTMBlock(
    (fullyconnection_block): Sequential(
      (0): Linear(in_features=256, out_features=1024, bias=True)
      (1): PReLU(num_parameters=1)
      (2): Linear(in_features=1024, out_features=256, bias=True)
      (3): PReLU(num_parameters=1)
      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (lstm): LSTM(256, 256, batch_first=True)
    (layernorm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (layernorm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lstmblock_3): LSTMBlock(
    (fullyconnection_block): Sequential(
      (0): Linear(in_features=256, out_features=1024, bias=True)
      (1): PReLU(num_parameters=1)
      (2): Linear(in_features=1024, out_features=256, bias=True)
      (3): PReLU(num_parameters=1)
      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (lstm): LSTM(256, 256, batch_first=True)
    (layernorm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (layernorm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lstmblock_4): LSTMBlock(
    (fullyconnection_block): Sequential(
      (0): Linear(in_features=256, out_features=1024, bias=True)
      (1): PReLU(num_parameters=1)
      (2): Linear(in_features=1024, out_features=256, bias=True)
      (3): PReLU(num_parameters=1)
      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (lstm): LSTM(256, 256, batch_first=True)
    (layernorm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (layernorm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask): Sequential(
    (0): Linear(in_features=256, out_features=2048, bias=True)
    (1): Sigmoid()
  )
  (decoder): ConvTranspose1d(2048, 1, kernel_size=(320,), stride=(160,))
  (fc): Linear(in_features=256, out_features=1955, bias=True)
)
Trainer: #param: 16.10M
Trainer: resume spk model from checkpoint /home/work_nfs4_ssd/hzhao/aslp-spknet-fork/exp/ecapa_augment_vox2/results/ecapa_augments_vox2/final.pth.tar
Trainer: gradient clipping by 5.0, default L2
Trainer: set eval mode...
successful to reload_spk:  /home/work_nfs4_ssd/hzhao/aslp-spknet-fork/exp/ecapa_augment_vox2/results/ecapa_augments_vox2/final.pth.tar
/home/work_nfs7/zqwang/summer_intern_dir/team03/summer_intern/model/ecapa_zhengju.py:319: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/aten/src/ATen/native/SpectralOps.cpp:590.)
  fft = torch.rfft(strided_wavform, 1, normalized=False, onesided=True)
Trainer: Epoch:0 processed 1.00e+02 / 2.86e+02 batches (snr_loss = +47.30)...
Trainer: Epoch:0 processed 1.00e+02 / 2.86e+02 batches (ce_loss = +7.58)...
Trainer: Epoch:0 processed 1.00e+02 / 2.86e+02 batches (mae_loss = +0.08)...
Trainer: Epoch:0 processed 1.00e+02 / 2.86e+02 batches (loss = +51.17)...
Trainer: Epoch:0 processed 2.00e+02 / 2.86e+02 batches (snr_loss = +47.37)...
Trainer: Epoch:0 processed 2.00e+02 / 2.86e+02 batches (ce_loss = +7.58)...
Trainer: Epoch:0 processed 2.00e+02 / 2.86e+02 batches (mae_loss = +0.08)...
Trainer: Epoch:0 processed 2.00e+02 / 2.86e+02 batches (loss = +51.24)...
Trainer: loss on 286 batches: 50.56,48.20,50.24,50.31,52.84,52.52,51.37,51.84,49.62,51.30,52.90,50.35,52.67,49.82,50.38,54.05,49.13,51.03,51.28,50.33,49.10,51.32,50.68,51.37,50.68,50.09,50.64,51.24,50.07,51.26,53.45,50.18,50.18,49.38,51.47,49.86,51.67,49.31,52.24,51.37,51.57,51.61,52.57,49.60,50.70,50.87,51.32,51.23,51.98,50.78,50.11,52.21,51.68,50.10,52.88,51.21,52.17,51.74,50.70,50.92,51.78,53.00,49.36,51.80,52.87,51.47,51.52,51.90,53.03,50.43,52.02,51.65,51.02,51.25,50.37,49.95,51.68,51.21,50.33,52.88,51.98,50.34,50.32,51.34,50.48,50.25,51.33,49.16,52.68,49.31,51.40,50.16,51.70,51.74,52.66,49.59,52.15,53.81,52.72,52.17,49.51,51.74,53.33,51.69,50.60,49.42,51.71,50.58,51.96,49.36,51.15,50.19,49.70,49.07,51.57,49.38,50.47,49.94,50.93,53.32,50.22,50.43,54.52,51.95,52.42,50.77,52.59,51.10,50.56,51.77,51.37,49.45,52.91,52.56,50.98,50.38,50.80,49.93,52.25,52.51,51.24,51.01,53.14,50.88,50.47,52.39,48.83,50.29,51.88,50.28,49.64,49.43,50.99,50.48,51.96,53.27,51.88,51.83,52.25,50.08,52.76,50.14,50.86,53.00,50.39,52.11,51.62,51.13,51.58,51.49,51.63,51.83,50.30,50.05,52.26,51.60,51.37,50.74,52.27,51.99,54.19,49.71,50.10,51.16,50.99,50.60,50.31,52.94,51.78,50.26,52.63,51.70,51.46,53.58,50.19,52.30,50.68,50.82,50.21,51.77,51.22,52.72,51.64,52.28,51.22,51.27,50.98,50.86,52.12,51.20,49.52,48.82,51.24,51.72,51.67,51.23,52.72,50.82,50.91,51.76,52.15,49.99,51.56,51.68,50.98,50.17,50.71,51.08,50.43,49.33,49.59,54.24,51.30,51.77,54.02,53.20,52.69,49.62,51.42,52.88,49.85,51.06,50.46,50.62,49.84,52.34,49.95,53.80,49.89,51.02,52.02,50.28,50.69,51.30,52.79,50.41,51.98,51.03,49.18,50.47,52.35,50.24,50.75,51.12,50.04,52.46,51.25,51.77,49.23,51.37,50.63,50.74,49.09,49.19,52.13,51.07,48.66,49.73,51.03,50.63,49.77,50.57,49.84,51.26,52.64,54.51
Trainer: start from epoch 0, loss = 51.1770
Trainer: set train mode...
Trainer: Epoch:1 processed 1.00e+02 / 1.18e+03 batches (snr_loss = -1.53)...
Trainer: Epoch:1 processed 1.00e+02 / 1.18e+03 batches (ce_loss = +6.68)...
Trainer: Epoch:1 processed 1.00e+02 / 1.18e+03 batches (mae_loss = +0.09)...
Trainer: Epoch:1 processed 1.00e+02 / 1.18e+03 batches (loss = +1.91)...
Trainer: Epoch:1 processed 1.00e+02 / 1.18e+03 batches (norm = +9.90)...
Trainer: Epoch:1 processed 2.00e+02 / 1.18e+03 batches (snr_loss = -3.75)...
Trainer: Epoch:1 processed 2.00e+02 / 1.18e+03 batches (ce_loss = +5.49)...
Trainer: Epoch:1 processed 2.00e+02 / 1.18e+03 batches (mae_loss = +0.04)...
Trainer: Epoch:1 processed 2.00e+02 / 1.18e+03 batches (loss = -0.96)...
Trainer: Epoch:1 processed 2.00e+02 / 1.18e+03 batches (norm = +7.34)...
Trainer: Epoch:1 processed 3.00e+02 / 1.18e+03 batches (snr_loss = -3.83)...
Trainer: Epoch:1 processed 3.00e+02 / 1.18e+03 batches (ce_loss = +5.00)...
Trainer: Epoch:1 processed 3.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 3.00e+02 / 1.18e+03 batches (loss = -1.30)...
Trainer: Epoch:1 processed 3.00e+02 / 1.18e+03 batches (norm = +9.80)...
Trainer: Epoch:1 processed 4.00e+02 / 1.18e+03 batches (snr_loss = -3.88)...
Trainer: Epoch:1 processed 4.00e+02 / 1.18e+03 batches (ce_loss = +4.81)...
Trainer: Epoch:1 processed 4.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 4.00e+02 / 1.18e+03 batches (loss = -1.45)...
Trainer: Epoch:1 processed 4.00e+02 / 1.18e+03 batches (norm = +10.83)...
Trainer: Epoch:1 processed 5.00e+02 / 1.18e+03 batches (snr_loss = -3.92)...
Trainer: Epoch:1 processed 5.00e+02 / 1.18e+03 batches (ce_loss = +4.72)...
Trainer: Epoch:1 processed 5.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 5.00e+02 / 1.18e+03 batches (loss = -1.54)...
Trainer: Epoch:1 processed 5.00e+02 / 1.18e+03 batches (norm = +11.72)...
Trainer: Epoch:1 processed 6.00e+02 / 1.18e+03 batches (snr_loss = -3.85)...
Trainer: Epoch:1 processed 6.00e+02 / 1.18e+03 batches (ce_loss = +4.64)...
Trainer: Epoch:1 processed 6.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 6.00e+02 / 1.18e+03 batches (loss = -1.50)...
Trainer: Epoch:1 processed 6.00e+02 / 1.18e+03 batches (norm = +7.11)...
Trainer: Epoch:1 processed 7.00e+02 / 1.18e+03 batches (snr_loss = -3.69)...
Trainer: Epoch:1 processed 7.00e+02 / 1.18e+03 batches (ce_loss = +4.58)...
Trainer: Epoch:1 processed 7.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 7.00e+02 / 1.18e+03 batches (loss = -1.37)...
Trainer: Epoch:1 processed 7.00e+02 / 1.18e+03 batches (norm = +9.67)...
Trainer: Epoch:1 processed 8.00e+02 / 1.18e+03 batches (snr_loss = -3.80)...
Trainer: Epoch:1 processed 8.00e+02 / 1.18e+03 batches (ce_loss = +4.56)...
Trainer: Epoch:1 processed 8.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 8.00e+02 / 1.18e+03 batches (loss = -1.49)...
Trainer: Epoch:1 processed 8.00e+02 / 1.18e+03 batches (norm = +11.63)...
Trainer: Epoch:1 processed 9.00e+02 / 1.18e+03 batches (snr_loss = -3.73)...
Trainer: Epoch:1 processed 9.00e+02 / 1.18e+03 batches (ce_loss = +4.53)...
Trainer: Epoch:1 processed 9.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 9.00e+02 / 1.18e+03 batches (loss = -1.43)...
Trainer: Epoch:1 processed 9.00e+02 / 1.18e+03 batches (norm = +8.71)...
Trainer: Epoch:1 processed 1.00e+03 / 1.18e+03 batches (snr_loss = -3.84)...
Trainer: Epoch:1 processed 1.00e+03 / 1.18e+03 batches (ce_loss = +4.48)...
Trainer: Epoch:1 processed 1.00e+03 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 1.00e+03 / 1.18e+03 batches (loss = -1.56)...
Trainer: Epoch:1 processed 1.00e+03 / 1.18e+03 batches (norm = +6.09)...
Trainer: Epoch:1 processed 1.10e+03 / 1.18e+03 batches (snr_loss = -3.78)...
Trainer: Epoch:1 processed 1.10e+03 / 1.18e+03 batches (ce_loss = +4.45)...
Trainer: Epoch:1 processed 1.10e+03 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 1.10e+03 / 1.18e+03 batches (loss = -1.52)...
Trainer: Epoch:1 processed 1.10e+03 / 1.18e+03 batches (norm = +9.40)...
Trainer: Loss(time/N, lr=1.000e-03) - Epoch  1: train = -1.1352(255.60m/1178)
Trainer: set eval mode...
Trainer: Epoch:1 processed 1.00e+02 / 2.86e+02 batches (snr_loss = -3.92)...
Trainer: Epoch:1 processed 1.00e+02 / 2.86e+02 batches (ce_loss = +9.77)...
Trainer: Epoch:1 processed 1.00e+02 / 2.86e+02 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 1.00e+02 / 2.86e+02 batches (loss = +0.99)...
Trainer: Epoch:1 processed 2.00e+02 / 2.86e+02 batches (snr_loss = -3.73)...
Trainer: Epoch:1 processed 2.00e+02 / 2.86e+02 batches (ce_loss = +9.77)...
Trainer: Epoch:1 processed 2.00e+02 / 2.86e+02 batches (mae_loss = +0.03)...
Trainer: Epoch:1 processed 2.00e+02 / 2.86e+02 batches (loss = +1.18)...
Trainer: loss on 286 batches: 1.75,1.10,2.07,1.66,1.92,1.06,1.78,0.07,0.59,-0.61,0.19,0.92,1.27,-0.22,-0.14,1.50,0.76,0.40,1.54,0.27,1.30,0.95,2.41,0.98,1.96,2.11,1.12,0.80,0.61,0.43,1.85,-0.00,0.79,0.67,1.52,2.61,0.92,-2.25,0.61,0.81,0.29,0.54,1.12,1.47,0.01,2.15,2.10,0.16,1.11,1.18,1.67,0.69,1.71,3.03,1.85,2.05,0.44,0.44,-1.97,0.82,0.97,0.27,0.59,0.65,1.28,0.55,2.02,1.43,-0.29,0.34,0.49,0.86,0.49,1.47,0.94,1.79,0.22,0.46,2.13,0.56,0.15,3.53,1.03,1.34,0.51,0.81,2.14,1.13,0.85,2.00,1.18,1.87,-0.73,2.11,-0.10,-0.50,2.00,0.42,2.58,0.83,1.30,0.84,1.38,0.16,0.78,1.38,0.63,1.52,1.69,1.34,0.28,1.89,1.27,1.90,0.20,1.68,1.04,2.34,-1.10,1.57,0.95,2.03,0.06,1.04,1.83,0.77,0.77,-0.87,1.75,2.00,2.61,-0.23,1.11,1.67,-0.08,0.56,1.36,1.90,1.83,0.28,1.94,0.01,3.80,2.13,0.53,0.37,2.02,0.68,1.07,1.47,0.29,0.87,1.92,1.31,2.05,1.92,-0.32,0.45,0.66,0.37,1.46,1.73,1.34,1.73,0.91,1.38,1.68,1.72,2.91,1.91,1.66,1.53,0.23,1.38,0.16,0.73,0.38,0.79,2.36,1.04,1.26,1.17,1.97,2.09,-0.15,0.63,0.24,1.83,-0.46,0.91,2.42,0.77,1.23,1.04,0.20,1.80,1.62,1.70,1.32,2.57,0.80,0.18,0.39,1.27,0.60,1.40,0.36,1.24,0.03,2.05,1.96,1.66,1.58,1.09,2.30,1.63,-1.26,0.36,-0.31,2.08,1.12,1.24,0.05,0.43,1.82,0.39,-0.34,1.23,0.87,0.92,1.75,1.10,-0.54,3.13,0.96,0.93,3.55,1.10,-0.18,0.37,1.13,1.05,1.08,1.53,1.95,1.98,-0.13,0.30,0.66,1.30,1.46,2.93,1.18,1.71,0.34,1.22,1.41,0.56,1.64,2.37,0.96,-0.03,2.96,1.34,1.51,0.67,0.07,1.45,1.62,1.40,1.13,1.53,0.43,1.88,2.73,1.38,0.63,0.91,1.83,1.42,0.49,0.28,0.38,1.64,-0.29,0.92
Trainer: save checkpoint best.pt.tar
Trainer: Loss(time/N, lr=1.000e-03) - Epoch  1: eval = 1.0898(22.29m/286)
/home/environment/zqwang/anaconda3/envs/py38pt17/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Trainer: save checkpoint last.pt.tar
Trainer: set train mode...
Trainer: Epoch:2 processed 1.00e+02 / 1.18e+03 batches (snr_loss = -3.78)...
Trainer: Epoch:2 processed 1.00e+02 / 1.18e+03 batches (ce_loss = +4.39)...
Trainer: Epoch:2 processed 1.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 1.00e+02 / 1.18e+03 batches (loss = -1.56)...
Trainer: Epoch:2 processed 1.00e+02 / 1.18e+03 batches (norm = +8.98)...
Trainer: Epoch:2 processed 2.00e+02 / 1.18e+03 batches (snr_loss = -3.81)...
Trainer: Epoch:2 processed 2.00e+02 / 1.18e+03 batches (ce_loss = +4.36)...
Trainer: Epoch:2 processed 2.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 2.00e+02 / 1.18e+03 batches (loss = -1.60)...
Trainer: Epoch:2 processed 2.00e+02 / 1.18e+03 batches (norm = +7.86)...
Trainer: Epoch:2 processed 3.00e+02 / 1.18e+03 batches (snr_loss = -3.89)...
Trainer: Epoch:2 processed 3.00e+02 / 1.18e+03 batches (ce_loss = +4.34)...
Trainer: Epoch:2 processed 3.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 3.00e+02 / 1.18e+03 batches (loss = -1.69)...
Trainer: Epoch:2 processed 3.00e+02 / 1.18e+03 batches (norm = +7.61)...
Trainer: Epoch:2 processed 4.00e+02 / 1.18e+03 batches (snr_loss = -3.84)...
Trainer: Epoch:2 processed 4.00e+02 / 1.18e+03 batches (ce_loss = +4.30)...
Trainer: Epoch:2 processed 4.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 4.00e+02 / 1.18e+03 batches (loss = -1.66)...
Trainer: Epoch:2 processed 4.00e+02 / 1.18e+03 batches (norm = +7.39)...
Trainer: Epoch:2 processed 5.00e+02 / 1.18e+03 batches (snr_loss = -3.80)...
Trainer: Epoch:2 processed 5.00e+02 / 1.18e+03 batches (ce_loss = +4.27)...
Trainer: Epoch:2 processed 5.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 5.00e+02 / 1.18e+03 batches (loss = -1.64)...
Trainer: Epoch:2 processed 5.00e+02 / 1.18e+03 batches (norm = +12.07)...
Trainer: Epoch:2 processed 6.00e+02 / 1.18e+03 batches (snr_loss = -3.71)...
Trainer: Epoch:2 processed 6.00e+02 / 1.18e+03 batches (ce_loss = +4.25)...
Trainer: Epoch:2 processed 6.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 6.00e+02 / 1.18e+03 batches (loss = -1.55)...
Trainer: Epoch:2 processed 6.00e+02 / 1.18e+03 batches (norm = +10.73)...
Trainer: Epoch:2 processed 7.00e+02 / 1.18e+03 batches (snr_loss = -3.76)...
Trainer: Epoch:2 processed 7.00e+02 / 1.18e+03 batches (ce_loss = +4.23)...
Trainer: Epoch:2 processed 7.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 7.00e+02 / 1.18e+03 batches (loss = -1.62)...
Trainer: Epoch:2 processed 7.00e+02 / 1.18e+03 batches (norm = +5.83)...
Trainer: Epoch:2 processed 8.00e+02 / 1.18e+03 batches (snr_loss = -3.95)...
Trainer: Epoch:2 processed 8.00e+02 / 1.18e+03 batches (ce_loss = +4.20)...
Trainer: Epoch:2 processed 8.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 8.00e+02 / 1.18e+03 batches (loss = -1.82)...
Trainer: Epoch:2 processed 8.00e+02 / 1.18e+03 batches (norm = +11.05)...
Trainer: Epoch:2 processed 9.00e+02 / 1.18e+03 batches (snr_loss = -3.76)...
Trainer: Epoch:2 processed 9.00e+02 / 1.18e+03 batches (ce_loss = +4.18)...
Trainer: Epoch:2 processed 9.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 9.00e+02 / 1.18e+03 batches (loss = -1.64)...
Trainer: Epoch:2 processed 9.00e+02 / 1.18e+03 batches (norm = +7.57)...
Trainer: Epoch:2 processed 1.00e+03 / 1.18e+03 batches (snr_loss = -3.98)...
Trainer: Epoch:2 processed 1.00e+03 / 1.18e+03 batches (ce_loss = +4.14)...
Trainer: Epoch:2 processed 1.00e+03 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 1.00e+03 / 1.18e+03 batches (loss = -1.88)...
Trainer: Epoch:2 processed 1.00e+03 / 1.18e+03 batches (norm = +5.87)...
Trainer: Epoch:2 processed 1.10e+03 / 1.18e+03 batches (snr_loss = -4.23)...
Trainer: Epoch:2 processed 1.10e+03 / 1.18e+03 batches (ce_loss = +4.11)...
Trainer: Epoch:2 processed 1.10e+03 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 1.10e+03 / 1.18e+03 batches (loss = -2.14)...
Trainer: Epoch:2 processed 1.10e+03 / 1.18e+03 batches (norm = +6.13)...
Trainer: Loss(time/N, lr=9.988e-04) - Epoch  2: train = -1.7357(215.10m/1178)
Trainer: set eval mode...
Trainer: Epoch:2 processed 1.00e+02 / 2.86e+02 batches (snr_loss = -4.10)...
Trainer: Epoch:2 processed 1.00e+02 / 2.86e+02 batches (ce_loss = +10.11)...
Trainer: Epoch:2 processed 1.00e+02 / 2.86e+02 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 1.00e+02 / 2.86e+02 batches (loss = +0.98)...
Trainer: Epoch:2 processed 2.00e+02 / 2.86e+02 batches (snr_loss = -4.22)...
Trainer: Epoch:2 processed 2.00e+02 / 2.86e+02 batches (ce_loss = +10.12)...
Trainer: Epoch:2 processed 2.00e+02 / 2.86e+02 batches (mae_loss = +0.03)...
Trainer: Epoch:2 processed 2.00e+02 / 2.86e+02 batches (loss = +0.87)...
Trainer: loss on 286 batches: 0.83,1.67,0.40,-0.17,0.73,-0.01,0.48,1.77,1.13,1.40,0.78,2.72,0.94,0.81,-0.20,1.34,0.20,2.00,1.32,1.88,2.40,0.90,2.58,-1.09,0.39,-0.12,0.60,-0.48,0.68,1.23,1.15,1.44,1.25,0.60,0.33,-0.23,1.77,0.38,1.63,1.21,2.06,-0.74,0.55,0.64,2.30,-0.40,1.10,2.41,0.11,-0.08,0.87,0.95,1.21,1.19,-1.32,2.13,0.53,0.13,1.59,0.96,3.02,2.26,0.26,4.06,2.67,-0.84,0.28,0.49,0.69,2.47,0.67,3.02,1.26,2.35,-0.23,1.30,0.57,0.17,2.09,1.79,0.60,1.42,0.38,0.03,1.27,1.54,1.18,1.32,-0.07,-0.55,1.32,1.60,0.20,-0.09,1.27,1.82,1.07,1.07,0.55,1.10,-0.78,0.26,1.08,2.78,-1.60,0.53,1.43,0.75,0.49,1.29,0.16,0.08,1.65,3.02,2.15,1.14,1.01,0.67,2.47,2.12,0.14,1.00,-0.97,0.92,1.91,-0.11,0.40,0.06,1.07,1.00,1.35,3.04,-0.03,0.19,1.70,0.90,0.57,0.67,0.94,1.39,0.92,1.47,0.13,0.27,0.29,0.07,1.11,0.50,1.42,-0.01,1.21,-0.01,0.45,2.97,1.43,0.43,2.38,0.76,-0.77,0.06,0.66,0.25,1.63,0.94,1.41,1.38,0.40,2.70,2.12,1.85,-0.30,0.01,-0.19,1.15,-0.19,0.28,0.54,1.59,0.77,-0.47,1.57,0.67,0.40,-0.23,-0.01,0.75,0.54,1.66,0.77,1.67,0.79,0.86,1.46,1.38,2.15,1.34,-1.38,1.04,1.14,1.96,0.20,1.27,-0.41,-0.71,-0.24,1.53,2.04,1.40,2.70,2.77,-0.23,0.67,1.49,1.06,-0.01,0.42,1.82,3.14,1.58,1.68,0.32,1.84,2.06,2.19,-0.25,1.16,1.12,-0.09,0.02,3.40,1.79,0.38,0.42,0.46,4.48,1.81,1.00,1.33,-0.01,0.93,2.51,1.49,0.25,1.13,1.40,-0.11,-0.43,1.53,2.80,1.06,-0.94,-0.40,0.99,1.09,0.31,-1.13,0.04,0.79,0.27,0.24,0.93,0.51,-0.06,1.77,0.15,0.81,1.58,0.60,0.09,1.21,0.89,1.12,-0.71,2.32,2.31,1.12,1.70,1.16,1.34,1.33,0.27,1.36,1.27,2.21,-0.05,0.42
Trainer: save checkpoint best.pt.tar
Trainer: Loss(time/N, lr=9.988e-04) - Epoch  2: eval = 0.9452(7.27m/286)
Trainer: save checkpoint last.pt.tar
Trainer: set train mode...
Trainer: Epoch:3 processed 1.00e+02 / 1.18e+03 batches (snr_loss = -4.25)...
Trainer: Epoch:3 processed 1.00e+02 / 1.18e+03 batches (ce_loss = +4.09)...
Trainer: Epoch:3 processed 1.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 1.00e+02 / 1.18e+03 batches (loss = -2.18)...
Trainer: Epoch:3 processed 1.00e+02 / 1.18e+03 batches (norm = +7.16)...
Trainer: Epoch:3 processed 2.00e+02 / 1.18e+03 batches (snr_loss = -4.18)...
Trainer: Epoch:3 processed 2.00e+02 / 1.18e+03 batches (ce_loss = +4.06)...
Trainer: Epoch:3 processed 2.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 2.00e+02 / 1.18e+03 batches (loss = -2.12)...
Trainer: Epoch:3 processed 2.00e+02 / 1.18e+03 batches (norm = +8.12)...
Trainer: Epoch:3 processed 3.00e+02 / 1.18e+03 batches (snr_loss = -4.19)...
Trainer: Epoch:3 processed 3.00e+02 / 1.18e+03 batches (ce_loss = +4.04)...
Trainer: Epoch:3 processed 3.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 3.00e+02 / 1.18e+03 batches (loss = -2.14)...
Trainer: Epoch:3 processed 3.00e+02 / 1.18e+03 batches (norm = +15.58)...
Trainer: Epoch:3 processed 4.00e+02 / 1.18e+03 batches (snr_loss = -4.39)...
Trainer: Epoch:3 processed 4.00e+02 / 1.18e+03 batches (ce_loss = +4.03)...
Trainer: Epoch:3 processed 4.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 4.00e+02 / 1.18e+03 batches (loss = -2.35)...
Trainer: Epoch:3 processed 4.00e+02 / 1.18e+03 batches (norm = +10.92)...
Trainer: Epoch:3 processed 5.00e+02 / 1.18e+03 batches (snr_loss = -4.28)...
Trainer: Epoch:3 processed 5.00e+02 / 1.18e+03 batches (ce_loss = +4.01)...
Trainer: Epoch:3 processed 5.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 5.00e+02 / 1.18e+03 batches (loss = -2.25)...
Trainer: Epoch:3 processed 5.00e+02 / 1.18e+03 batches (norm = +9.65)...
Trainer: Epoch:3 processed 6.00e+02 / 1.18e+03 batches (snr_loss = -4.29)...
Trainer: Epoch:3 processed 6.00e+02 / 1.18e+03 batches (ce_loss = +3.99)...
Trainer: Epoch:3 processed 6.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 6.00e+02 / 1.18e+03 batches (loss = -2.26)...
Trainer: Epoch:3 processed 6.00e+02 / 1.18e+03 batches (norm = +7.83)...
Trainer: Epoch:3 processed 7.00e+02 / 1.18e+03 batches (snr_loss = -4.42)...
Trainer: Epoch:3 processed 7.00e+02 / 1.18e+03 batches (ce_loss = +3.97)...
Trainer: Epoch:3 processed 7.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 7.00e+02 / 1.18e+03 batches (loss = -2.41)...
Trainer: Epoch:3 processed 7.00e+02 / 1.18e+03 batches (norm = +14.71)...
Trainer: Epoch:3 processed 8.00e+02 / 1.18e+03 batches (snr_loss = -4.35)...
Trainer: Epoch:3 processed 8.00e+02 / 1.18e+03 batches (ce_loss = +3.95)...
Trainer: Epoch:3 processed 8.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 8.00e+02 / 1.18e+03 batches (loss = -2.35)...
Trainer: Epoch:3 processed 8.00e+02 / 1.18e+03 batches (norm = +11.59)...
Trainer: Epoch:3 processed 9.00e+02 / 1.18e+03 batches (snr_loss = -4.42)...
Trainer: Epoch:3 processed 9.00e+02 / 1.18e+03 batches (ce_loss = +3.93)...
Trainer: Epoch:3 processed 9.00e+02 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 9.00e+02 / 1.18e+03 batches (loss = -2.43)...
Trainer: Epoch:3 processed 9.00e+02 / 1.18e+03 batches (norm = +13.10)...
Trainer: Epoch:3 processed 1.00e+03 / 1.18e+03 batches (snr_loss = -4.32)...
Trainer: Epoch:3 processed 1.00e+03 / 1.18e+03 batches (ce_loss = +3.89)...
Trainer: Epoch:3 processed 1.00e+03 / 1.18e+03 batches (mae_loss = +0.03)...
Trainer: Epoch:3 processed 1.00e+03 / 1.18e+03 batches (loss = -2.35)...
Trainer: Epoch:3 processed 1.00e+03 / 1.18e+03 batches (norm = +8.87)...
